\include{settings}

\begin{document}

\include{titlepage}

%\tableofcontents
%\listoffigures
%\newpage
%
%\section{Цели работы}
%
%Изучение и приобретение навыков:
%\begin{itemize}
%	\item инициализации НС;
%	\item использования функций обработки входных/выходных сигналов;
%	\item задания различных функций расчета производительности НС;
%	\item задания различных функций деления выборки;
%	\item практического применения сетей прямого распространения (НСПР);
%	\item обучения НСПР по алгоритму обратного распространения (ОР).
%\end{itemize}

\section{Изучение вспомогательных функций}

\subsection{Изучение функций инициализации}

%1. С помощью команды network создайте НС с n входами, 1 выходным слоем с m нейронами.
%Задайте функцию инициализации НС initlay.
%2. Задайте функцию инициализации слоев initnw. Произведите инициализацию НС с помощью функции init. Проанализируйте матрицы смещений и весов нейронной сети.
%3. Задайте функцию инициализации слоев initwb.
%Задавайте для весов и смещений различные функции инициализации (initzero, rands, randsmall – и для весов и для смещений, midpoint, randnc, randnr, initlvq, initsompc – только для весов, initcon – только для смещений). Произведите инициализацию НС с помощью функции init. Проанализируйте матрицы смещений и весов нейронной сети.

Создадим нейронную сеть с 3 входами и одним выходным слоем, содержащим 2 нейрона. Таким образом имеем 3 коэффициента входного слоя \code{net.IW\{1,1\}}, 2 коэффициента весов между входным и выходным слоями \code{net.LW\{2,1\}} и по 1 коэффиценту смещения на каждом слое \code{net.b\{1\}} и \code{net.b\{1\}}, т.е. всего 7 коэффициентов.

\lstinputlisting[linerange={7-10}]{lab3_1_1.m}

Зададим функцию инициализации слоев \code{'initnw'}. Эта функция генерирует начальные веса и смещения для слоя так, чтобы активные области нейронов были распределены равномерно относительно области входов, что обеспечивает минимизацию числа нейронов сети и времени обучения.

\lstinputlisting[linerange={19-24}]{lab3_1_1.m}

В результате коэффиценты оказались равны:
\begin{gather*}
IW_{1,1} = \begin{bmatrix} -0.5254 & -0.0823 & 0.9262 \end{bmatrix}^T,\
b_1 = \begin{bmatrix} 0.0936 & 0.0423 & -0.5368 \end{bmatrix}^T \\
LW_{2,1} = \begin{bmatrix} -0.0222 & 0.3583 & -0.2651 \\ 0.2481 & -0.2090 & 0.9760 \end{bmatrix},\ 
b_2 = \begin{bmatrix} -0.9245 \\ 0.7703 \end{bmatrix}
\end{gather*}

Зададим функцию инициализации слоев \code{'initwb'}, которая позволяет использовать собственные функции инициализации для каждой матрицы весов и каждого вектора смещений. 

\lstinputlisting[linerange={26-35}]{lab3_1_1.m}

Зададим различные функции инициализации коэффицентов, например при \code{iw11 = 'rands'}, \code{b1 = 'randsmall'}, \code{lw11 = 'initlvq'} и \code{b2 = 'initzero'} коэффициенты оказались равны:
\begin{gather*}
IW_{1,1} = \begin{bmatrix} -0.4763 & -0.3293 & 0.3595 \end{bmatrix}^T,\
b_1 = \begin{bmatrix} 0.0004 & -0.0081 & 0.0064 \end{bmatrix}^T \\
LW_{2,1} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \end{bmatrix},\ 
b_2 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\end{gather*}

Таким образом были рассмотрены различные функции инициализации коэффициентов и смещений слоев нейронной сети.

\subsection{Изучение функций предобработки входов/выходов}

%Задавайте в качестве функций предобработки различные встроенные функции (fixunknowns, mapminmax, mapstd, processpca, removeconstantrows, removerows) – по очереди по одной.
%Для каждой из функций предобработки подберите актуальные входные примеры, на которые эти функции должны срабатывать (для fixunknowns например задайте в качестве одного из входных сигналов NaN). Подайте эти примеры на вход этих функций и проанализируйте обработанные функциями примеры.

Рассмотрим работу основных встроенных функций преобработки.
\begin{itemize}
	\item \code{fixunknowns(X)} помечает строки, содержащие \code{NaN} и заменят такие элементы на среднее значение в строке. Например, подать матрицу \code{X = [1 2 3; 4 NaN 6]}, то функция вернет на одну строку больше: \code{[1 2 3; 4 5 6; 1 0 1]}. В дополнительной строке содержится ноль, если соответствующей ей элемент в предыдущей строке был равен \code{NaN}.
	\item \code{mapminmax(X)} приводит все значения к интервалу $[-1, 1]$. Например, если подать матрицу \code{X = [1 2 3; 6 5 4]}, то функция вернет \code{[-1 0 1; 1 0 -1]}.
	\item \code{mapstd(X)} приводит все строки к такому виду, что среднее строки равно 0, а стандартное отклонение равно 1. Например, если подать матрицу \code{X = [1 2 3; 4 5 6]}, то функция вернет \code{[-1 0 1; -1 0 1]}.
	\item \code{processpca(X)} проводит обработку столбцов матрицы с анализом основных компонент. Например, если подать матрицу \code{X = [1 2 3; 4 5 6]}, то функция вернет \code{[-4.0758 -5.3845 -6.6931; 0.6229 0.0869 -0.4492]}.
	\item \code{removeconstantrows(X)} удаляет из матрицы константные. Например, если подать матрицу \code{X = [3 3 3; 1 2 3]}, то функция вернет \code{[1 2 3]}.
	\item \code{removerows(X, idx)} удаляет из матрицы строки с индексами, содерщаимися в \code{idx}. Например, если подать матрицу \code{X = [1 2 3; 4 5 6; 7 8 9], [1, 3])}, то функция вернет \code{[4 5 6]}.
\end{itemize}

\newpage

\subsection{Изучение функций расчета производительности (усреднения ошибки)}

%Сформируйте различные вектора (матрицы) ошибок E (интерпретируйте их как вектора ошибок НС на тестовой выборке). Размерность E = число выходов * число векторов. Последовательно примените к ним функции mae, mse, sae, sse. Проанализируйте каждый из способов усреднения ошибки (можно ли ему доверять и когда – при каком числе выходов НС и при каком объеме тестовой выборки). Рассмотрите случаи, когда ошибки сильно рознятся по входам/примерам.

Сформируем искусственный вектор ошибок E и будем интерпретировать его как вектор ошибок НС на тестовой выборке. Создадим матрицу размером $2 \times 5$, т.е. содержащую по 2 выходных значения нейронной сети для каждого из 5 тестовых примеров.
\begin{equation*}
E = \begin{bmatrix}
	0 & 1 & 2 & 0 & 30 \\
	10 & 0 & 15 & 0 & 2
\end{bmatrix}
\end{equation*}

Для данной матрицы ошибок значения функции ошибок равны:
\begin{multicols}{4}
	\centering
	\code{mae} = 6 \\
	\code{mse} = 123.4 \\
	\code{sae} = 60 \\
	\code{sse} = 1234
\end{multicols}

Значения функций \code{sae} и \code{sse} в 10 раз (размерность матрицы) больше, чем значения соответсвутющих им функций \code{mae} и \code{mse}, так как значения ошибок в данном случае не усредяются. Можно заметить, что функции \code{mse} и \code{sse} гораздо более чувствительны к выбросам (элемент 30 в данном случае), соответственно им можно доверять при большом объеме выборки, когда данные выбросы будут нивелированы.

\subsection{Изучение функций деления выборки}

%Для объема выборки Q = 100 примените функции деления выборки divideblock, divideind, divideint, dividerand, dividetrain. Ознакомьтесь с входными параметрами каждой из функций и при исследовании функций меняйте эти параметры. Проанализируйте, как формируются обучающая, тестовая и контрольная выборки для каждой из функций.

Сформируем выборку размером 100 (\code{P = 0.01 : 0.01 : 1}) и применим к ней различные функции для ее деления:
\begin{itemize}
	\item \code{[Ptr, Pval, Ptest] = divideblock(P, 0.5, 0.3, 0.2)} делит выборку блоками, т.е. первые 50\% выборки будет принадлежать множеству \code{Ptr}, следюущие за этим блоком 30\% выборки будет принадлежать \code{Pval} и т.д.
	\item \code{[Ptr, Pval, Ptest] = divideind(P, 1:50, 51:80, 81:100)} делит выборку путем задания индексов каждой подвыборки, т.е. подмножеству \code{Ptr} будут принадлежать примеры с индексами от 1 до 50 и т.д.
	\item \code{[Ptr, Pval, Ptest] = divideint(P, 0.5, 0.3, 0.2)} делит выборку перемежающимися блоками, т.е. элементы будут последовательно добавляться в одно из подмножеств.
	\item \code{[Ptr, Pval, Ptest] = dividerand(P, 0.5, 0.3, 0.2)} делит выборку на подмножества случайным образом.
	\item \code{[Ptr, Pval, Ptest] = dividetrain(P, 0.5, 0.3, 0.2)} не делит выборку на подмножества, а добавляет все относит все примеры к тренировочному подмножеству.
\end{itemize}

\section{Аппроксимация статических зависимостей}

Сформируем обучающую и тестовую выборки на основе ранее созданных функций. На рис. \ref{fig:2_2} изображены сформированные выборки.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=1]{2_2}
	\caption{Обучающая и тестовая выборки}
	\label{fig:2_2}
\end{center}
\end{figure}

Создадим нейронную сеть с одним скрытым слоем, содержащим 20 нейронов, и инициализируем ее с помощью случайных значений. Зададим ей основные параметры обучения.

\lstinputlisting[linerange={28-37}]{lab3_2_1.m}

Обучим нейронную сеть на обучающей выборке. На рис. \ref{fig:2_4} изображена зависимость ошибки от номера эпохи.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_4}
	\caption{Зависимость ошибки от номера эпохии}
	\label{fig:2_4}
\end{center}
\end{figure}

Теперь будем изменять количество нейронов в скрытом слое в диапазоне $1 \div 50$. 

\lstinputlisting[linerange={16-26}]{lab3_2_1.m}

На рис. \ref{fig:2_5_1} изображена зависимость ошибки от числа нейронов в скрытом слое.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_5_1}
	\caption{Зависимость ошибки от числа нейронов в скрытом слое}
	\label{fig:2_5_1}
\end{center}
\end{figure}

Из графика видно, что оптимальное количество нейронов в скрытом слое лежит в диапазоне $10 \div 20$. На рис. \ref{fig:2_5_2} изображены аппроксимации при различном количестве нейронов.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_5_2}
	\caption{Аппроксимации при различном количестве нейронов}
	\label{fig:2_5_2}
\end{center}
\end{figure}
\vspace{-0.5cm}

Применим различные пакетные алгоритмы обучения и подберем для каждого из них оптимальные параметры. В таблице \ref{tab:2_6} приведены сводные результаты.

\begin{table}[H]
\begin{center}
	\def\tabcolsep{15pt}
	\caption{Функции обучения}
	\label{tab:2_6}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Функция & Параметры & Число нейронов & Ошибка & Число эпох \\
		\hline
		\hline
		\code{traingda} & - & - & - & - \\
		\hline
		\code{traingdm} & - & - & - & - \\
		\hline
		\code{traingdx} & - & - & - & - \\
		\hline
		\code{trainrp} & - & - & - & - \\
		\hline
		\hline
		\code{traincgf} & - & - & - & - \\
		\hline
		\code{trainscg} & - & - & - & - \\
		\hline
		\hline
		\code{trainbfg} & - & - & - & - \\
		\hline
		\code{trainlm} & - & - & - & - \\
		\hline
		\code{trainoss} & - & - & - & - \\
		\hline
	\end{tabular}
\end{center}
\end{table} 

\section{Выводы}

В данной работе были приобретены навыки построения и обучения линейных сетей для классификации сложных образов и линейной аппроксимации. Была изучена архитектура статических и динамических линейных нейронных сетей и специальных функций для их создания.

\end{document}