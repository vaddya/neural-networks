\include{settings}

\begin{document}

\include{titlepage}

\tableofcontents
\newpage
\listoftables
\listoffigures
\newpage

\section{Цели работы}

\begin{itemize}
	\item Приобретение навыков построения, инициализации и обучения РФБ-НС.
	\item Исследование РФБ-НС при решении задач аппроксимации статических зависимостей и классификации.
\end{itemize}

\section{Аппроксимация статических зависимостей}

\subsection{Формирование обучающей и тестовой выборки}

%Задайте достаточное для точной аппроксимации количество обучающих примеров (в данном случае совпадающее с числом РБФ-нейронов).

Сформируем обучающую и тестовую выборки на основе ранее созданных функций. На рис. \ref{fig:2_1} изображены сформированные выборки. Исходная выборка из $100$ примеров была разделена на обучающую и тестовую в соотношении $0.7 : 0.3$.

\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_1}
	\caption{Обучающая и тестовая выборки}
	\label{fig:2_1}
\end{center}
\end{figure}

\subsection{Аппроксимация с помощью точной РБФ-НС}

%Изменяя ширину РБФ (spread), определите наилучшее значение этого параметра с точки зрения качества аппроксимации. Постройте три графика аппроксимации для разных значений spread: оптимального, больше и меньше оптимального, когда явно видно, что аппроксимация плохая. При построении графиков аппроксимации не забывайте приводить графики исходной желаемой зависимости.
%Постройте график зависимости ошибки аппроксимации от параметра spread в логарифмическом масштабе.
%С помощью точной РБФ-НС (newrbe) аппроксимируйте функцию (для вашего варианта из заданий с номером 5).

Подберем наилучшее значение параметра \code{spread} для обучения с помощью точной РБФ-НС. На рис. \ref{fig:2_2_1} изображена зависимость ошибки \code{perform} от значения параметра \code{spread} в логарифмическом масштабе. 
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_2_1}
	\caption{Зависимость \code{perform} от \code{spread}}
	\label{fig:2_2_1}
\end{center}
\end{figure}

Оптимальным можно считать значения \code{spread} $\leq 0.1$. При таком значении параметра ошибка аппроксимации $\sim 10^{-20}$. Постром два графика аппроксимации для оптимального значения \code{spread} и больше оптимального. При уменьшении параметра даже до $10^{-10}$ функция аппроксимируется практически без ошибок. На рис. \ref{fig:2_2_2} изображены построенные аппроксимации.
\begin{figure}[H]
\begin{center}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_2_2}
		\caption{\code{spread = 1}}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_2_3}
		\caption{\code{spread = 0.1}}
	\end{subfigure}
	\caption{Аппроксимация при разных значениях \code{spread}}
	\label{fig:2_2_2}
\end{center}
\end{figure}

\subsection{Аппроксимация с помощью приближенной РБФ-НС}

%Изменяя значение допустимой ошибки (goal), постройте зависимость числа используемых РБФ-нейронов от допустимой ошибки аппроксимации. Для промежуточных результатов постройте графики аппроксимации.

На рис. \ref{fig:2_3_1} изображена зависимость числа используемых РБФ-нейронов от допустимой ошибки аппроксимации \code{goal} в логарифмическом масштабе.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_3_1}
	\caption{Зависимость числа нейронов от \code{goal}}
	\label{fig:2_3_1}
\end{center}
\end{figure}

На рис. \ref{fig:2_3_2} изображены аппроксимации при разных значениях допустимой ошибки.
\begin{figure}[H]
\begin{center}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_3_2}
		\caption{\code{goal = 0.01} (2 нейрона)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_3_3}
		\caption{\code{goal = 0.001} (18 нейронов)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_3_4}
		\caption{\code{goal = 0.0001} (23 нейрона)}
	\end{subfigure}
	\caption{Аппроксимация при разных значениях \code{goal}}
	\label{fig:2_3_2}
\end{center}
\end{figure}

\subsection{Аппроксимация с помощью GRNN}

%По аналогии с newrb изменяя ширину РБФ (spread), определите наилучшее значение этого параметра с точки зрения качества аппроксимации. Постройте график аппроксимации на исходной зависимости.
%Уменьшите объем обучающей выборки в несколько раз (рассмотрите 3 случая) и подберите оптимальные значения параметра spread для каждого случая. Постройте полученные графики аппроксимации. Приведите значения ошибок.

Подберем наилучшее значение параметра \code{spread} для обучения с помощью GRNN. На рис. \ref{fig:2_4_1} изображена зависимость ошибки \code{perform} от значения параметра \code{spread} в логарифмическом масштабе. 
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{2_4_1}
	\caption{Зависимость \code{perform} от \code{spread}}
	\label{fig:2_4_1}
\end{center}
\end{figure}

Оптимальным можно считать значения \code{spread} $\leq 0.01$. Постром два графика аппроксимации для оптимального значения \code{spread} и больше оптимального. При уменьшении параметра даже до $10^{-10}$ функция аппроксимируется без ошибок. На рис. \ref{fig:2_4_2} изображены построенные аппроксимации.
\begin{figure}[H]
\begin{center}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_4_2}
		\caption{\code{spread = 0.1}}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{2_4_3}
		\caption{\code{spread = 0.01}}
	\end{subfigure}
	\caption{Аппроксимация при разных значениях \code{spread}}
	\label{fig:2_4_2}
\end{center}
\end{figure}

\subsection{Сравнение качества апрпоксимации}

%Сравните качество аппроксимации сетями прямого распространения и различными вариантами РБФ-НС (точной, приближенной РБФ-НС и GRNN) по различным показателям:
%- число нейронов, требуемое для достижения заданного качества аппроксимации;
%- время обучения;
%- сложность настройки (выбора параметров) НС.
%Постройте на одном графике зависимости ошибки от числа нейронов для различных типов НС.

По сравнению с сетями прямого распространения, РБФ сети могут добится в разы более низкой ошикби аппроксимации: $\sim 10^{-20}$ против $\sim 10^{-6}$. Однако, в точной РБС-НС количество нейронов равняется обучающей выборке, в то время как в НСПР требовалось около $20 \div 30$ нейронов для аппроксимации данной зависимости. В приближенной РБФ-НС и GRNN сетях количество используемых нейронов напрямую зависит от допустимой ошибки: для допустимой ошибки $\sim 10^{-2}$ требуется всего 2 нейрона, для $\sim 10^{-5}$ около 20, а при $\sim 10^{-10}$ все 100 нейронов, что соответствует размеру выборки. РБФ-НС имеют сложности в настройке, так как от выбора параметра \code{spread} напрямую зависит качество аппроксимации, однако и НСПР имеют свои сложности:  необходимо задавать количество нейронов скрытого слоя и парметры функции обучения.

\section{Разбиение плоскости на $2$ класса}

\subsection{Формирование обучающей и тестовой выборки}

%Задайте достаточное для точной классификации количество обучающих примеров.

Сформируем обучающую и тестовую выборки на основе ранее созданных функций. На рис. \ref{fig:3_1} изображены сформированные выборки.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=1]{3_1}
	\caption{Обучающая и тестовая выборки}
	\label{fig:3_1}
\end{center}
\end{figure}

\subsection{Подбор значения spread}

%Подберите оптимальное значение spread в смысле минимальной ошибки на тестовой выборке. Визуализируйте результаты классификации (диаграмма соотнесения тестовых примеров с классами, раскраска плоскости). Приведите значение средней ошибки.
%Постройте дополнительно графики классификации для значений spread, больших и меньших оптимального, когда явно видно, что классификация неудовлетворительная.

Подберем наилучшее значение параметра \code{spread} для обучения с помощью точной РБФ-НС. На рис. \ref{fig:3_2_1} изображена зависимость ошибки \code{perform} от значения параметра \code{spread} в логарифмическом масштабе. 
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=0.9]{3_2_1}
	\caption{Зависимость \code{perform} от \code{spread}}
	\label{fig:3_2_1}
\end{center}
\end{figure}

Оптимальным можно считать значения \code{spread} $\leq 0.01$. При таком значении параметра ошибка аппроксимации равна нулю. Постром два графика аппроксимации для оптимального значения \code{spread} и больше оптимального. При уменьшении параметра нулевая ошибка уже не уменшится. На рис. \ref{fig:3_2_2} изображены построенные аппроксимации. При \code{spread} $\geq 0.5$ PNN всегда выдает нулевой класс.
\begin{figure}[H]
\begin{center}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{3_2_2}
		\caption{\code{spread = 0.2}}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{3_2_3}
		\caption{\code{spread = 0.01}}
	\end{subfigure}
	\caption{Аппроксимация при разных значениях \code{spread}}
	\label{fig:3_2_2}
\end{center}
\end{figure}

\subsection{Уменьшение объема выборки}

%Уменьшите объем обучающей выборки в несколько раз (рассмотрите 3 случая) и подберите оптимальные значения параметра spread для каждого случая. Визуализируйте результаты классификации и приведите значение средней ошибки.

\subsection{Сравнение качества классификации}

%Постройте поверхность ошибки в плоскости двух параметров: ширина РБФ-функции spread и объем обучающей выборки.
%Сравните полученные результаты с НС прямого распространения по аналогии с п. 3 задания 1.

\newpage

\section{Разбиение плоскости на $N$ классов}

\subsection{Формирование обучающей и тестовой выборки}

Сформируем обучающую и тестовую выборки на основе ранее созданных функций. На рис. \ref{fig:4_1} изображены сформированные выборки: обучающая выборка обозначена ноликами, а тестовая -- крестиками.
\begin{figure}[H]
\begin{center}
	\includegraphics[scale=1]{4_1}
	\caption{Обучающая и тестовая выборки}
	\label{fig:4_1}
\end{center}
\end{figure}

\subsection{Подбор значения spread}

%Подберите оптимальное значение spread в смысле минимальной ошибки на тестовой выборке. Визуализируйте результаты классификации (диаграмма соотнесения тестовых примеров с классами, раскраска плоскости). Приведите значение средней ошибки.
%Постройте дополнительно графики классификации для значений spread, больших и меньших оптимального, когда явно видно, что классификация неудовлетворительная.
%Дополнительно приведите матрицы неточностей и рассчитайте ошибки первого и второго родов.

\subsection{Уменьшение объема выборки}

%Уменьшите объем обучающей выборки в несколько раз (рассмотрите 3 случая) и подберите оптимальные значения параметра spread для каждого случая. Визуализируйте результаты классификации и приведите значение средней ошибки.

\subsection{Сравнение качества классификации}

%Постройте поверхность ошибки в плоскости двух параметров: ширина РБФ-функции spread и объем обучающей выборки.
%Сравните полученные результаты с НС прямого распространения по аналогии с п. 3 задания 1.

\newpage

\section{Классификация многомерных образов}

\subsection{Формирование обучающей и тестовой выборки}

%Сформируйте обучающую выборку достаточного объема.

На рис. \ref{fig:5_1} изображены примеры обучающих и тестовых образов.
\begin{figure}[H]
\begin{center}
	%\includegraphics[scale=1]{5_1}
	\caption{Примеры обучающей и тестовай выборки}
	\label{fig:5_1}
\end{center}
\end{figure}

\subsection{Подбор значения spread}

%Подберите оптимальное значение spread.

\subsection{Качество классификации}

%Исследуйте качество классификации на тестовой выборке, содержащей зашумленные примеры. Приведите матрицу неточностей, рассчитайте среднюю ошибку и ошибки 1,2 родов.

\subsection{Изменение объема выборки}

%Попробуйте увеличить и уменьшить объем обучающей выборки в несколько раз. Для каждого случая подберите оптимальное значение spread и рассчитайте показатели качества классификации.
%Проанализируйте полученные результаты, сравнив их с результатами классификации тех же самых образов НС прямого распространения по аналогии с п. 3 задания 1.

\section{Выводы}

В данной работе были приобретены навыки построения, инициализации и обучения нейронных сетей с радиально-базисными функциями для решения задач аппроксимации статической аппроксимации и классификации. Были изучены точные РБФ-НС, приближенные РБФ-НС и GRNN.

\end{document}